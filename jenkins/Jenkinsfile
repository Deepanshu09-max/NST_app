pipeline {
  agent any

  environment {
    GITHUB_REPO_URL         = 'https://github.com/Deepanshu09-max/NST_app.git'
    GIT_BRANCH              = 'devops'
    DOCKER_CREDS            = 'dockerhub-credentials'
    MINIKUBE_PROFILE        = 'devops'
    FINE_TUNE_JOB           = 'NST_Fine_Tune_Job'
    BAD_FEEDBACK_THRESHOLD  = '5'  // Keep as string so .toInteger() works cleanly
    HOST_PERSISTENT_STORAGE = '/home/deepanshu/Documents/SPE Major/NST_app/persistent_storage'
  }

  options {
    timestamps()
    buildDiscarder(logRotator(numToKeepStr: '10'))
  }

  stages {
    //â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    stage('Checkout') {
      steps {
        git branch: "${GIT_BRANCH}", url: "${GITHUB_REPO_URL}"
      }
    }

    //â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    stage('Ensure Minikube & Mount') {
      steps {
        sh '''
          SRC="${HOST_PERSISTENT_STORAGE}"
          PROFILE="${MINIKUBE_PROFILE}"

          echo "â–¶ Killing any prior minikube mount"
          pkill -f "minikube mount --profile=${PROFILE}" || true

          echo "â†’ Checking for existing profile â€˜${PROFILE}â€™"
          if ! minikube profile list | grep -q "${PROFILE}"; then
            echo "â†’ No profile found; starting â€˜${PROFILE}â€™"
            minikube start --driver=docker --profile="${PROFILE}"
          else
            echo "â†’ Using existing profile â€˜${PROFILE}â€™"
            minikube status --profile="${PROFILE}"
          fi

          echo "â–¶ Mounting host â†’ Minikube:/persistent_storage"
          nohup minikube mount --profile="${PROFILE}" \
              "${SRC}:/persistent_storage" \
              --uid=$(id -u) --gid=$(id -g) \
            > minikube-mount.log 2>&1 &

          sleep 5
          pgrep -f "minikube mount --profile=${PROFILE}" \
            && echo "âœ” Mount OK" \
            || ( echo "âŒ Mount failed"; cat minikube-mount.log; exit 1 )

          echo "â–¶ Dumping kubeconfig for Ansible"
          minikube -p "${PROFILE}" kubectl -- config view --raw \
            > "$WORKSPACE/minikube-kubeconfig.yaml"

          if [ ! -s "$WORKSPACE/minikube-kubeconfig.yaml" ]; then
            echo "âŒ Failed to dump kubeconfig"
            exit 1
          fi
          echo "âœ… kubeconfig dumped at $WORKSPACE/minikube-kubeconfig.yaml"
        '''
        stash name: 'kubecfg', includes: 'minikube-kubeconfig.yaml'
      }
    }

    //â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    stage('Build & Up Services (Compose inside Minikube)') {
      steps {
        sh '''
          echo "â–¶ Switching Docker CLI to Minikubeâ€™s daemon"
          eval $(minikube -p "${MINIKUBE_PROFILE}" docker-env)
          echo "â–¶ Confirming Docker is now Minikubeâ€™s:"
          docker info | grep "Server Version" -m1
        '''

        script {
          def dockerfileDiff = sh(
            script: "git diff --name-only origin/${GIT_BRANCH} HEAD | grep -E 'Dockerfile|.dockerignore' || true",
            returnStdout: true
          ).trim()

          if (dockerfileDiff) {
            echo "âš ï¸ Detected Dockerfile changes; tearing down Minikube compose containers"
            sh 'docker-compose -p nstapp -f docker-compose.yml down -v --remove-orphans'
          } else {
            echo "âœ… No Dockerfile changes; skipping tear down"
          }
        }

        sh '''
          echo "ğŸš€ Building & launching services inside Minikubeâ€™s Dockerâ€¦"
          docker inspect deepanshu0903/nst_app:fine_tuning_service
          docker-compose -p nstapp -f docker-compose.yml up -d --build
          echo "âœ… Compose up finished (should be fast if no changes)."
          docker ps | grep nstapp_
        '''
      }
    }

    //â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    stage('Integration Tests') {
      steps {
        sh '''
          echo "â–¶ Running integration tests"
          python3 test.py || true
        '''
      }
    }

    //â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    stage('Tag & Push Images (Store & Compare IDs)') {
      steps {
        script {
          // 1) Path to the JSON file where we store lastâ€‘pushed ImageIDs
          def idsFile = "${env.HOST_PERSISTENT_STORAGE}/image_ids.json"
    
          // 2) If the file does not exist or is empty, reinitialize it to a valid JSON object
          if (!fileExists(idsFile) || readFile(idsFile).trim() == '') {
            echo "âš™ï¸  Creating or reâ€‘initializing ${idsFile} â†’ {}"
            writeFile file: idsFile, text: '{}'
          }
    
          // 3) (Optional) Debug: show exactly whatâ€™s inside image_ids.json before parsing
          echo "ğŸ” [DEBUG] contents of ${idsFile}:"
          sh "cat ${idsFile} || echo '<file-not-found or empty>'"
    
          // 4) Parse the JSON (now guaranteed to be at least â€œ{}â€)
          def storedIds
          try {
            storedIds = readJSON file: idsFile
          } catch (Exception err) {
            error("âŒ Could not parse ${idsFile} as JSON: ${err.getMessage()}")
          }
    
          // 5) List of all service tags
          def services = [
            'tf-base',
            'frontend',
            'routing_service',
            'fine_tuning_service',
            'inference_service_model1',
            'inference_service_model2',
            'inference_service_model3',
            'inference_service_model4'
          ]
    
          // 6) Docker Hub credentials
          withCredentials([usernamePassword(
            credentialsId: "${DOCKER_CREDS}",
            usernameVariable: 'DOCKER_USER',
            passwordVariable: 'DOCKER_PASS'
          )]) {
            services.each { svc ->
              def localTag  = "deepanshu0903/nst_app:${svc}"
              def remoteTag = "${env.DOCKER_USER}/nst_app:${svc}"
    
              echo ">>> Checking '${svc}'"
    
              // a) Does the local image exist?
              def localExists = sh(
                script: "docker image inspect ${localTag} >/dev/null 2>&1 && echo 'YES' || echo 'NO'",
                returnStdout: true
              ).trim()
    
              if (localExists != 'YES') {
                echo "âŒ Local image '${localTag}' not found; skipping."
                return
              }
    
              // b) Get the current local ID
              def localId = sh(
                script: "docker image inspect --format='{{.Id}}' ${localTag}",
                returnStdout: true
              ).trim()
    
              // c) Look up the previously stored ID (or empty if missing)
              def prevId = storedIds[svc] ?: ''
              echo "    Local ID: ${localId}"
              echo "    PrevÂ Stored ID: ${prevId ?: '<none>'}"
    
              if (prevId == localId) {
                echo "â– ${svc} unchanged (IDs match); skipping push"
              } else {
                echo "ğŸ”– ${svc} changed (or firstâ€time); pushingâ€¦"
    
                // d) Tag & push
                sh """
                  echo "$DOCKER_PASS" | docker login -u "$DOCKER_USER" --password-stdin
                  docker tag "${localTag}" "${remoteTag}"
                  docker push "${remoteTag}"
                """.stripIndent()
    
                // e) After successful push, update storedIds[svc] = localId
                storedIds[svc] = localId
    
                // f) Write the updated map back to the JSON file
                writeJSON file: idsFile, json: storedIds, pretty: true
                echo "    âœï¸ Updated stored ID for '${svc}' â†’ ${localId}"
              }
            } // end services.each
    
            echo "âœ…Â All 'store & compare IDs' tasks finished"
          } // end withCredentials
        } // end script
      } // end steps
    } // end stage


    //â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    stage('Check Feedback & Trigger Fine-Tuning') {
      steps {
        script {
          // Paths to files (note spaces in â€œSPE Majorâ€ are already quoted via $HOST_PERSISTENT_STORAGE)
          def fbFile     = "${env.HOST_PERSISTENT_STORAGE}/feedback.jsonl"
          def offsetFile = "${env.HOST_PERSISTENT_STORAGE}/feedback.offset"
          def countsFile = "${env.HOST_PERSISTENT_STORAGE}/feedback.counts.json"

          // 1) Verify feedback.jsonl exists
          echo "ğŸ” [DEBUG] Looking for feedback file at: ${fbFile}"
          if (!fileExists(fbFile)) {
            echo "â„¹ï¸  No feedback file found at '${fbFile}' â†’ skipping fineâ€‘tune check."
            return
          }

          // 2) Read all lines from feedback.jsonl
          def allLines = []
          try {
            allLines = readFile(fbFile).readLines()
          } catch (err) {
            echo "âŒ  Failed to read '${fbFile}': ${err.getMessage()}"
            return
          }
          def totalLines = allLines.size()
          echo "ğŸ” [DEBUG] totalLines = ${totalLines}"

          // 3) Determine last offset (default to 0 if missing/invalid)
          int offsetValue = 0
          if (fileExists(offsetFile)) {
            try {
              offsetValue = readFile(offsetFile).trim().toInteger()
              echo "ğŸ” [DEBUG] Parsed offsetValue = ${offsetValue}"
            } catch (Exception e) {
              echo "âš ï¸  Invalid offset in '${offsetFile}'; resetting to 0."
              offsetValue = 0
            }
          } else {
            echo "ğŸ” [DEBUG] No offset file at '${offsetFile}'; defaulting offsetValue = 0"
          }

          // If offsetValue > totalLines (file truncated), reset to 0
          if (offsetValue > totalLines) {
            echo "âš ï¸  offsetValue (${offsetValue}) > totalLines (${totalLines}); resetting offset â†’ 0"
            offsetValue = 0
          }

          // 4) If no new lines, update offset and exit
          if (totalLines <= offsetValue) {
            echo "âœ…  No new feedback (offset=${offsetValue}, totalLines=${totalLines})."
            writeFile file: offsetFile, text: totalLines.toString()
            echo "ğŸ” [DEBUG] Wrote new offset (${totalLines}) to '${offsetFile}'"
            return
          }

          // 5) Compute newLines array
          def newLines = allLines[offsetValue..<totalLines]
          echo "â„¹ï¸  Found ${newLines.size()} new line(s) of feedback"

          // 6) Load existing cumulative â€œbadâ€ counts
          def cumulativeCounts = [:]
          if (fileExists(countsFile)) {
            try {
              cumulativeCounts = readJSON file: countsFile
              echo "ğŸ” [DEBUG] Loaded cumulativeCounts = ${cumulativeCounts}"
            } catch (Exception e) {
              echo "âš ï¸  Could not parse '${countsFile}'; initializing all counts to 0."
              cumulativeCounts = [:]
            }
          } else {
            echo "ğŸ” [DEBUG] No counts file at '${countsFile}'; initializing all counts to 0."
          }

          // 7) Count â€œbadâ€ feedback in newLines per model
          def newBadCounts = [:].withDefault { 0 }
          newLines.eachWithIndex { line, idx ->
            def text = line?.trim()
            if (!text) {
              echo "âš ï¸  newLines[${idx}] empty or whitespace, skipping"
              return
            }
            try {
              echo "ğŸ” [DEBUG] Parsing JSON on newLines[${idx}]"
              def json = readJSON(text: text)
              echo "ğŸ” [DEBUG] Parsed: feedback='${json.feedback}', model='${json.model}'"
              if (json.feedback == 'bad' && json.model) {
                newBadCounts[json.model] += 1
              }
            } catch (Exception e) {
              echo "âŒ  JSON parse error on newLines[${idx}]: ${e.getMessage()}"
              echo "ğŸ” [DEBUG] Raw line content: ${text}"
            }
          }

          // 8) Merge newBadCounts into cumulativeCounts and collect models to trigger
          def threshold = env.BAD_FEEDBACK_THRESHOLD.toInteger()
          def toTrigger = []
          newBadCounts.each { model, badThisRun ->
            int oldCount = cumulativeCounts.containsKey(model) ? (cumulativeCounts[model] as Integer) : 0
            int updatedCount = oldCount + badThisRun
            cumulativeCounts[model] = updatedCount
            echo "ğŸ“Š  Model='${model}', oldBad=${oldCount}, newBad=${badThisRun}, totalBad=${updatedCount}"
            if (updatedCount >= threshold) {
              toTrigger << model
            }
          }

          // 9) Trigger fineâ€‘tune for any model that hit threshold, then reset its count
          def anyTriggered = false
          toTrigger.unique().each { model ->
            echo "â–¶ Triggering fineâ€‘tune for '${model}'"
            build job: "${FINE_TUNE_JOB}",
                  wait: false,
                  parameters: [
                    string(name: 'BAD_FEEDBACK_COUNT', value: cumulativeCounts[model].toString()),
                    string(name: 'MODEL_NAME',         value: model)
                  ]
            cumulativeCounts[model] = 0
            echo "â„¹ï¸  Reset cumulativeCounts['${model}'] â†’ 0"
            anyTriggered = true
          }

          if (!anyTriggered) {
            echo "âœ…  No model exceeded threshold (${threshold}); no fineâ€‘tune triggered."
          }

          // 10) Write updated counts back
          writeJSON file: countsFile, json: cumulativeCounts
          echo "ğŸ” [DEBUG] Wrote updated cumulativeCounts to '${countsFile}': ${cumulativeCounts}"

          // 11) Update offset
          writeFile file: offsetFile, text: totalLines.toString()
          echo "ğŸ” [DEBUG] Updated '${offsetFile}' to ${totalLines}"
        } // end script
      } // end steps
    } // end stage
  } // end stages

  post {
    success {
      echo 'âœ… Pipeline completed successfully.'
    }
    failure {
      echo 'âŒ Pipeline failed â€” please inspect the logs.'
    }
    always {
      sh '''
        echo "â–¶ Killing minikube mount"
        pkill -f "minikube mount" || true
      '''
    }
  }
}
